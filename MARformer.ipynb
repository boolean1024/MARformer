{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Token_performer(nn.Module):\n",
    "    def __init__(self, dim, in_dim, head_cnt=1, kernel_ratio=0.5, dp1=0.1, dp2 = 0.1):\n",
    "        super().__init__()\n",
    "        self.emb = in_dim * head_cnt \n",
    "        self.kqv = nn.Linear(dim, 3 * self.emb)\n",
    "        self.dp = nn.Dropout(dp1)\n",
    "        self.proj = nn.Linear(self.emb, self.emb)\n",
    "        self.head_cnt = head_cnt\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(self.emb)\n",
    "        self.epsilon = 1e-8 \n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.emb, 1 * self.emb),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1 * self.emb, self.emb),\n",
    "            nn.Dropout(dp2),\n",
    "        )\n",
    "\n",
    "        self.m = int(self.emb * kernel_ratio)\n",
    "        self.w = torch.randn(self.m, self.emb)\n",
    "        self.w = nn.Parameter(nn.init.orthogonal_(self.w) * math.sqrt(self.m),\n",
    "                              requires_grad=False)\n",
    "\n",
    "    def prm_exp(self, x):\n",
    "\n",
    "        xd = ((x * x).sum(dim=-1, keepdim=True)).repeat(1, 1, self.m) / 2\n",
    "        wtx = torch.einsum('bti,mi->btm', x.float(), self.w)\n",
    "\n",
    "        return torch.exp(wtx - xd) / math.sqrt(self.m)\n",
    "\n",
    "    def single_attn(self, x):\n",
    "        k, q, v = torch.split(self.kqv(x), self.emb, dim=-1)\n",
    "        kp, qp = self.prm_exp(k), self.prm_exp(q)  \n",
    "        D = torch.einsum('bti,bi->bt', qp, kp.sum(dim=1)).unsqueeze(dim=2) \n",
    "        kptv = torch.einsum('bin,bim->bnm', v.float(), kp) \n",
    "        y = torch.einsum('bti,bni->btn', qp, kptv) / (D.repeat(1, 1, self.emb) + self.epsilon) \n",
    "        # skip connection\n",
    "        y = v + self.dp(self.proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.single_attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, in_dim = None, qkv_bias=False, qk_scale=None,\n",
    "                 attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, in_dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.in_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.in_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        x = v.squeeze(1) + x \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Token_transformer(nn.Module):\n",
    "    def __init__(self, dim, in_dim, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, in_dim=in_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(in_dim)\n",
    "        self.mlp = Mlp(in_features=in_dim, hidden_features=int(in_dim*mlp_ratio),\n",
    "                       out_features=in_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "def intra_att_f(querys,keys,values): \n",
    "    len = querys.shape[-1]\n",
    "    att = torch.zeros(querys.shape)\n",
    "    for q in range(querys.shape[0]):\n",
    "        tmp_q = torch.zeros(len, len)\n",
    "        for k in keys:\n",
    "            tmp = querys[q].reshape(len,1) @ k.reshape(1,len) * (torch.ones(len, len) - torch.eye(len, len))\n",
    "            tmp_q += tmp\n",
    "\n",
    "        att[q] = torch.sum(tmp_q, dim=0).reshape((1,-1))\n",
    "\n",
    "    return values*att\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "                 proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class intra_att(nn.Module): \n",
    "    def __init__(self,dim=64):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        print(qkv.shape)\n",
    "        querys, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        len = querys.shape[-1]\n",
    "        att = torch.zeros(querys.shape)\n",
    "        for q in range(querys.shape[0]):\n",
    "            tmp_q = torch.zeros(len, len)\n",
    "            for k in keys:\n",
    "                tmp = querys[q].reshape(len,1) @ k.reshape(1,len) * (torch.ones(len, len) - torch.eye(len, len))\n",
    "                tmp_q += tmp\n",
    "        att[q] = torch.sum(tmp_q, dim=0).reshape((1,-1))\n",
    "\n",
    "        return values*att\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0.,\n",
    "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_sinusoid_encoding(n_position, d_hid):\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n",
    "                for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiHeadDense(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(MultiHeadDense, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ch, out_ch))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.weight)\n",
    "        return x\n",
    "\n",
    "\n",
    "class T2T_module(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=64, tokens_type='performer', in_chans=1, embed_dim=256, token_dim=64, kernel=32, stride=32):\n",
    "        super().__init__()\n",
    "\n",
    "        if tokens_type == 'transformer':\n",
    "            print('adopt transformer encoder for tokens-to-token')\n",
    "            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(2, 2))\n",
    "            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(1, 1),dilation=(2,2))\n",
    "            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "            self.attention1 = Token_transformer(dim=in_chans*7*7, in_dim=token_dim,\n",
    "                                                num_heads=1, mlp_ratio=1.0)\n",
    "            self.attention2 = Token_transformer(dim=token_dim*3*3, in_dim=token_dim,\n",
    "                                                num_heads=1, mlp_ratio=1.0)\n",
    "            self.project = nn.Linear(token_dim * 3 * 3, embed_dim)\n",
    "\n",
    "        elif tokens_type == 'performer':\n",
    "            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(2, 2))\n",
    "            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(1, 1),dilation=(2,2))\n",
    "            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "            self.attention1 = Token_performer(dim=in_chans*7*7, in_dim=token_dim,\n",
    "                                              kernel_ratio=0.5)\n",
    "            self.attention2 = Token_performer(dim=token_dim*3*3, in_dim=token_dim,\n",
    "                                              kernel_ratio=0.5)\n",
    "            self.project = nn.Linear(token_dim * 3 * 3, embed_dim)\n",
    "\n",
    "        self.num_patches = 529 \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.soft_split0(x)\n",
    "        \n",
    "\n",
    "        x = self.attention1(x.transpose(1, 2))\n",
    "        res_11 = x\n",
    "        B, new_HW, C = x.shape\n",
    "        x = x.transpose(1,2).reshape(B, C, int(np.sqrt(new_HW)), int(np.sqrt(new_HW)))\n",
    "        x = torch.roll(x, shifts=(2, 2), dims=(2, 3)) \n",
    "        x = self.soft_split1(x)\n",
    "        \n",
    "\n",
    "        x = self.attention2(x.transpose(1, 2))\n",
    "        res_22 = x\n",
    "        B, new_HW, C = x.shape\n",
    "        x = x.transpose(1, 2).reshape(B, C, int(np.sqrt(new_HW)), int(np.sqrt(new_HW)))\n",
    "        x = torch.roll(x, shifts=(2, 2), dims=(2, 3))\n",
    "        x = self.soft_split2(x)\n",
    "\n",
    "        x = self.project(x.transpose(1, 2))\n",
    "        return x,res_11,res_22\n",
    "\n",
    "\n",
    "class Token_back_Image(nn.Module):\n",
    "    def __init__(self, img_size=64, tokens_type='performer', in_chans=1, embed_dim=256, token_dim=64, kernel=32, stride=32):\n",
    "        super().__init__()\n",
    "\n",
    "        if tokens_type == 'transformer':\n",
    "            print('adopt transformer encoder for tokens-to-token')\n",
    "            self.soft_split0 = nn.Fold((64,64),kernel_size=(7, 7), stride=(2, 2))\n",
    "            self.soft_split1 = nn.Fold((29,29),kernel_size=(3, 3), stride=(1, 1),dilation=(2,2))\n",
    "            self.soft_split2 = nn.Fold((25,25),kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "            self.attention1 = Token_transformer(dim=token_dim, in_dim=in_chans*7*7, num_heads=1, mlp_ratio=1.0)\n",
    "            self.attention2 = Token_transformer(dim=token_dim, in_dim=token_dim*3*3, num_heads=1, mlp_ratio=1.0)\n",
    "            self.project = nn.Linear(embed_dim,token_dim * 3 * 3)\n",
    "        elif tokens_type == 'performer':\n",
    "            self.soft_split0 = nn.Fold((64,64),kernel_size=(7, 7), stride=(2, 2))\n",
    "            self.soft_split1 = nn.Fold((29,29),kernel_size=(3, 3), stride=(1, 1),dilation=(2,2))\n",
    "            self.soft_split2 = nn.Fold((25,25),kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "            self.attention1 = Token_performer(dim=token_dim, in_dim=in_chans*7*7, kernel_ratio=0.5)\n",
    "            self.attention2 = Token_performer(dim=token_dim, in_dim=token_dim*3*3, kernel_ratio=0.5)\n",
    "            self.project = nn.Linear(embed_dim,token_dim * 3 * 3)\n",
    "\n",
    "        self.num_patches = (img_size // (1 * 2 * 2)) * (img_size // (1 * 2 * 2))\n",
    "\n",
    "    def forward(self, x, res_11,res_22):    \n",
    "        x = self.project(x).transpose(1, 2) \n",
    "\n",
    "        x = self.soft_split2(x)\n",
    "        x = torch.roll(x, shifts=(-2, -2), dims=(-1, -2))\n",
    "        x = rearrange(x,'b c h w -> b c (h w)').transpose(1,2)\n",
    "        x = x + res_22\n",
    "        x = self.attention2(x).transpose(1, 2)\n",
    "        \n",
    "        x = self.soft_split1(x)\n",
    "        x = torch.roll(x, shifts=(-2, -2), dims=(-1, -2))\n",
    "        x = rearrange(x,'b c h w -> b c (h w)').transpose(1,2)\n",
    "        x = x + res_11\n",
    "        x = self.attention1(x).transpose(1, 2)\n",
    "        \n",
    "        x = self.soft_split0(x) \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MARformer(nn.Module):\n",
    "    def __init__(self, img_size=512, tokens_type='convolution', in_chans=1, num_classes=1000,\n",
    "                 embed_dim=768, depth=12,\n",
    "                 num_heads=12, kernel=32, stride=32, mlp_ratio=4., qkv_bias=False,\n",
    "                 qk_scale=None, drop_rate=0.1, attn_drop_rate=0.1,\n",
    "                 drop_path_rate=0.1, norm_layer=nn.LayerNorm, token_dim=1024):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim \n",
    "\n",
    "        self.tokens_to_token = T2T_module(\n",
    "                img_size=img_size, tokens_type=tokens_type, in_chans=in_chans,\n",
    "            embed_dim=embed_dim, token_dim=token_dim,kernel=kernel, stride=stride)\n",
    "        num_patches = self.tokens_to_token.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(n_position=num_patches,\n",
    "                                      d_hid=embed_dim), requires_grad=False)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # CTformer decoder\n",
    "        self.dconv1 = Token_back_Image(img_size=img_size, tokens_type=tokens_type, in_chans=in_chans, \n",
    "                                       embed_dim=embed_dim, token_dim=token_dim, kernel=kernel, stride=stride)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        x, res_11, res_22 = self.tokens_to_token(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        i = 0\n",
    "        for blk in self.blocks:\n",
    "            i += 1\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        out = res1 - self.dconv1(x,res_11,res_22)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from utils.plot_util import Animator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(net, train_iter, val_iter, num_epochs, lr, device, loss, lr_period, lr_decay, pretrainModel=None):\n",
    "\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    if pretrainModel is None:\n",
    "        net.apply(init_weights)\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(pretrainModel))\n",
    "        \n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_period, lr_decay)\n",
    "    animator = Animator(xlabel='epoch', legend=['train loss', 'val loss', 'val ssim', 'val psnr'], figsize=(4.0, 3.0))\n",
    "    num_batches = len(train_iter)\n",
    "    \n",
    "    ssim_max, psnr_max = 0.0, 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        metric = d2l.Accumulator(2)\n",
    "        net.train()\n",
    "        train_l = 0.0\n",
    "        val_l = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with tqdm(train_iter,\n",
    "                      desc=\"train epoch {}/{} \".format(epoch + 1, num_epochs),\n",
    "                      postfix={'loss': train_l}) as tbar:\n",
    "            for i, (X, y) in enumerate(tbar):\n",
    "                optimizer.zero_grad()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = net(X)\n",
    "                \n",
    "                l = loss(y_hat, y) * 100 + 1e-4\n",
    "                l.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    metric.add(l * X.shape[0], X.shape[0])\n",
    "\n",
    "                train_l = metric[0] / metric[1]\n",
    "                if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                    animator.add(epoch + (i + 1) / num_batches, (train_l, None, None, None))\n",
    "            \n",
    "                tbar.set_postfix({'loss': train_l})\n",
    "                tbar.update()\n",
    "            tbar.close()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        val_l, ssim, psnr = validate(net, val_iter, device, loss, evaluate)\n",
    "\n",
    "        animator.add(epoch + 1, (None, val_l, ssim * 10, psnr))\n",
    "\n",
    "        print(\"epoch [{}/{}], train loss: {:.3f}, val loss: {:.3f}, val ssim: {:.4f}, val psnr: {:.2f}, time of this epoch: {:.1f}s\"\n",
    "                .format(epoch + 1, num_epochs, train_l, val_l, ssim, psnr, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "\n",
    "def evaluate(imgs1, imgs2):\n",
    "    imgs1 = imgs1.numpy().astype(np.float64)\n",
    "    imgs2 = imgs2.numpy().astype(np.float64)\n",
    "    print(img1.shape)\n",
    "\n",
    "    ssim = 0.0\n",
    "    psnr = 0.0\n",
    "    for img1,img2 in zip(imgs1,imgs2):\n",
    "        ssim += compare_ssim(img1, img2, channel_axis=0)\n",
    "        psnr += compare_psnr(imgs1, imgs2, data_range=1.0)\n",
    "    \n",
    "    return ssim, psnr\n",
    "\n",
    "\n",
    "def validate(net, val_iter, device, loss ,evaluate):\n",
    "    metric = d2l.Accumulator(4)\n",
    "    net.eval()\n",
    "\n",
    "    ssim_avg = 0.0\n",
    "    psnr_avg = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(val_iter):\n",
    "            arrs = split_arr(x, 64).to(device)\n",
    "            arrs[0:64] = net(arrs[0:64])\n",
    "            arrs[64:2 * 64] = net(arrs[64:2 * 64])\n",
    "            arrs[2 * 64:3 * 64] = net(arrs[2 * 64:3 * 64])\n",
    "            arrs[3 * 64:4 * 64] = net(arrs[3 * 64:4 * 64])\n",
    "            y_hat = agg_arr(arrs, 256).to('cpu')\n",
    "            \n",
    "            l = loss(y_hat, y) * 100 + 1e-4\n",
    "            \n",
    "            ssim,psnr = evaluate(y_hat.cpu(), y.cpu())\n",
    "\n",
    "            metric.add(l * y.shape[0], ssim, psnr, y.shape[0])\n",
    "            \n",
    "            val_l = metric[0] / metric[3]\n",
    "            ssim_avg = metric[1] / metric[3]\n",
    "            psnr_avg = metric[2] / metric[3]\n",
    "\n",
    "    return val_l, ssim_avg, psnr_avg    \n",
    "\n",
    "\n",
    "def test(net, test_iter, device, model=None):\n",
    "\n",
    "    if model is not None:\n",
    "        net.load_state_dict(torch.load(model))\n",
    "        \n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    \n",
    "    print('testing on', device)\n",
    "    \n",
    "    metric = d2l.Accumulator(3)\n",
    "    \n",
    "    ssim_avg = 0.0\n",
    "    psnr_avg = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            \n",
    "            arrs = split_arr(x, 64).to(device)\n",
    "            arrs[0:64] = net(arrs[0:64])\n",
    "            arrs[64:2 * 64] = net(arrs[64:2 * 64])\n",
    "            arrs[2 * 64:3 * 64] = net(arrs[2 * 64:3 * 64])\n",
    "            arrs[3 * 64:4 * 64] = net(arrs[3 * 64:4 * 64])\n",
    "            y_hat = agg_arr(arrs, 256).to('cpu')\n",
    "            \n",
    "            \n",
    "            ssim,psnr = evaluate(y_hat.cpu(), y.cpu())\n",
    "\n",
    "            metric.add(ssim, psnr, y.shape[0])\n",
    "\n",
    "            ssim_avg = metric[0] / metric[2]\n",
    "            psnr_avg = metric[1] / metric[2]\n",
    "\n",
    "\n",
    "    print(\"test ssim: {:.4f}, test psnr: {:.2f}, time of this test: {:.1f}s\"\n",
    "                .format(ssim_avg, psnr_avg, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def data_augmentation(image, mode):\n",
    "    out = image\n",
    "    if mode == 0:\n",
    "        out = out\n",
    "    elif mode == 1:\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 2:\n",
    "        out = np.rot90(out)\n",
    "    elif mode == 3:\n",
    "        out = np.rot90(out)\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 4:\n",
    "        out = np.rot90(out, k=2)\n",
    "    elif mode == 5:\n",
    "        out = np.rot90(out, k=2)\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 6:\n",
    "        out = np.rot90(out, k=3)\n",
    "    elif mode == 7:\n",
    "        out = np.rot90(out, k=3)\n",
    "        out = np.flipud(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_patch(full_input_img, full_target_img, patch_n, patch_size):\n",
    "    assert full_input_img.shape == full_target_img.shape\n",
    "    patch_input_imgs = []\n",
    "    patch_target_imgs = []\n",
    "    h, w = full_input_img.shape\n",
    "    new_h, new_w = patch_size, patch_size\n",
    "    if patch_size == h:\n",
    "        return full_input_img, full_target_img\n",
    "\n",
    "    for _ in range(patch_n // 2):\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        patch_input_img = full_input_img[top:top + new_h, left:left + new_w]\n",
    "        patch_target_img = full_target_img[top:top + new_h, left:left + new_w]\n",
    "\n",
    "        patch_input_imgs.append(patch_input_img)\n",
    "        patch_target_imgs.append(patch_target_img)\n",
    "\n",
    "        \n",
    "        tmp = np.random.randint(1, 8)\n",
    "        patch_input_img = data_augmentation(patch_input_img, tmp)\n",
    "        patch_target_img = data_augmentation(patch_target_img, tmp)\n",
    "\n",
    "        patch_input_imgs.append(patch_input_img)\n",
    "        patch_target_imgs.append(patch_target_img)\n",
    "\n",
    "    return np.array(patch_input_imgs), np.array(patch_target_imgs)\n",
    "\n",
    "\n",
    "def split_arr(arr, patch_size, stride=32):\n",
    "    pad = (16, 16, 16, 16)\n",
    "    arr = nn.functional.pad(arr, pad, \"constant\", 0)\n",
    "    _, _, h, w = arr.shape\n",
    "    num = h // stride - 1\n",
    "    arrs = torch.zeros(num * num, 1, patch_size, patch_size)\n",
    "\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            arrs[i * num + j, 0] = arr[0, 0, i * stride:i * stride + patch_size,\n",
    "                                   j * stride:j * stride + patch_size]\n",
    "    return arrs\n",
    "\n",
    "\n",
    "def agg_arr(arrs, size, stride=32):\n",
    "    arr = torch.zeros(size, size)\n",
    "    n, _, h, w = arrs.shape\n",
    "    num = size // stride\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            arr[i * stride:(i + 1) * stride, j * stride:(j + 1) * stride] = arrs[i * num + j, :, 16:48, 16:48]\n",
    "    # return arr\n",
    "    return arr.unsqueeze(0).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.file_util import read_dir\n",
    "import os.path as path\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "from random import choice\n",
    "\n",
    "\n",
    "class DeepLesionDataset(Dataset):\n",
    "    def __init__(self, mode, dataset_dir, partial_holdout=0.2, hu_offset=32768):\n",
    "        self.mode = mode\n",
    "        self.partial_holdout = partial_holdout\n",
    "        self.hu_offset = hu_offset\n",
    "        self.train_list = []\n",
    "        self.validate_list = []\n",
    "        self.test_list = []\n",
    "\n",
    "        self.pre_process(dataset_dir)\n",
    "\n",
    "    def pre_process(self, dataset_dir):\n",
    "        data_dict = {}\n",
    "        cache_name = ''\n",
    "        dest_dir = ''\n",
    "        if 'train' == self.mode or 'validate' == self.mode:\n",
    "            cache_name = 'train_cache.json'\n",
    "            dest_dir = dataset_dir + '/train'\n",
    "        elif 'test' == self.mode:\n",
    "            cache_name = 'test_cache.json'\n",
    "            dest_dir = dataset_dir + '/test'\n",
    "\n",
    "        if type(dataset_dir) is str and path.isdir(dataset_dir):\n",
    "            cache_file = path.join(dataset_dir, cache_name)\n",
    "            if path.isfile(cache_file):\n",
    "                with open(cache_file) as f:\n",
    "                    data_dict = json.load(f)\n",
    "            else:\n",
    "                gt_files = read_dir(dest_dir, predicate=lambda x: x == \"gt.mat\", recursive=True)\n",
    "\n",
    "                for gt_file in gt_files:\n",
    "                    metal_dir = path.split(gt_file)[0]\n",
    "                    metal_files = sorted(read_dir(metal_dir, predicate=lambda x: x.endswith(\"mat\") and x != \"gt.mat\"))\n",
    "                    data_dict[gt_file] = [f for f in metal_files]\n",
    "\n",
    "                with open(cache_file, 'w') as f:\n",
    "                    json.dump(data_dict, f)\n",
    "\n",
    "        data_dict = sorted(data_dict.items())\n",
    "\n",
    "        if 'train' == self.mode or 'validate' == self.mode:\n",
    "            if self.partial_holdout:\n",
    "                train_size = int(len(data_dict) * (1 - self.partial_holdout))\n",
    "                self.train_list = data_dict[:train_size]\n",
    "                self.validate_list = data_dict[train_size:]\n",
    "            else:\n",
    "                self.train_list = data_dict\n",
    "                self.validate_list = []\n",
    "        elif 'test' == self.mode:\n",
    "            self.test_list = data_dict\n",
    "\n",
    "    def convert2coefficient(self, image, MIUWATER=0.192):\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        image = image - self.hu_offset\n",
    "        image[image < -1000] = -1000\n",
    "        image = image / 1000 * MIUWATER + MIUWATER\n",
    "        return image\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        with_art = sio.loadmat(data_file[0])['image']\n",
    "        gt = sio.loadmat(data_file[1])['image']\n",
    "        gt = self.convert2coefficient(gt).T\n",
    "        return with_art, gt\n",
    "\n",
    "    def normalize(self, data, min=0.0, max=0.5):\n",
    "        data = np.clip(data, min, max)\n",
    "        data = (data - min) / (max - min)\n",
    "        data = data * 2.0 - 1.0\n",
    "        return data\n",
    "\n",
    "    def denormalize(self, data, min=0.0, max=0.5):\n",
    "        data = data * 0.5 + 0.5\n",
    "        data = data * (max - min) + min\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        if 'train' == self.mode:\n",
    "            return len(self.train_list)\n",
    "        elif 'validate' == self.mode:\n",
    "            return len(self.validate_list)\n",
    "        elif 'test' == self.mode:\n",
    "            return len(self.test_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if 'train' == self.mode:\n",
    "            gt_file, art_files = self.train_list[idx]\n",
    "\n",
    "        elif 'validate' == self.mode:\n",
    "            gt_file, art_files = self.validate_list[idx]\n",
    "\n",
    "        elif 'test' == self.mode:\n",
    "            gt_file, art_files = self.test_list[idx]\n",
    "        \n",
    "        data_file = choice(art_files), gt_file\n",
    "        art_img, gt = self.load_data(data_file)\n",
    "        art_img, gt = self.normalize(art_img), self.normalize(gt)\n",
    "        \n",
    "        \n",
    "        if 'train' == self.mode:\n",
    "            art_img, gt = get_patch(art_img, gt, patch_n=4, patch_size=64)\n",
    "            \n",
    "        art_img, gt = torch.tensor(art_img), torch.tensor(gt)\n",
    "\n",
    "        return art_img, gt\n",
    "\n",
    "    \n",
    "    def get_dataset(self, batch_size=8, shuffle=True, num_workers=10):\n",
    "        return DataLoader(dataset=self, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
